# æ—¥è¯­è¯æ±‡å’Œè¯­æ³•æ•°æ®çˆ¬å–æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•è·å–å¤§é‡æ—¥è¯­å­¦ä¹ æ•°æ®,åŒ…æ‹¬è¯æ±‡ã€è¯­æ³•ã€ä¾‹å¥ç­‰ã€‚

---

## ğŸ“š æ–¹æ³•ä¸€: ä½¿ç”¨ç°æˆçš„å¼€æºæ•°æ®é›† (æœ€ç®€å•,æ¨è)

### ä¼˜ç‚¹:
- âœ… æ— éœ€ç¼–ç¨‹
- âœ… æ•°æ®è´¨é‡é«˜
- âœ… åˆæ³•åˆè§„
- âœ… ç«‹å³å¯ç”¨

### æ¨èæ•°æ®æº:

#### 1. **JMdict (æ—¥è‹±è¯å…¸é¡¹ç›®)**
- **ç½‘å€**: https://www.edrdg.org/jmdict/j_jmdict.html
- **å†…å®¹**: 18ä¸‡+æ—¥è¯­è¯æ±‡,åŒ…å«è¯»éŸ³ã€é‡Šä¹‰ã€è¯æ€§
- **æ ¼å¼**: XML/JSON
- **ä¸‹è½½**: 
  ```bash
  wget http://ftp.edrdg.org/pub/Nihongo/JMdict_e.gz
  gunzip JMdict_e.gz
  ```

#### 2. **JLPTè¯æ±‡è¡¨ (GitHub)**
- **ç½‘å€**: https://github.com/stephenmk/JMdictDB
- **å†…å®¹**: æŒ‰JLPTç­‰çº§åˆ†ç±»çš„è¯æ±‡
- **æ ¼å¼**: JSON/CSV
- **ä¸‹è½½**: ç›´æ¥å…‹éš†ä»“åº“

#### 3. **Tatoeba (ä¾‹å¥æ•°æ®åº“)**
- **ç½‘å€**: https://tatoeba.org/zh-hans/downloads
- **å†…å®¹**: 100ä¸‡+æ—¥è¯­ä¾‹å¥,å¸¦ä¸­æ–‡ç¿»è¯‘
- **æ ¼å¼**: TSV
- **ä¸‹è½½**:
  ```bash
  wget https://downloads.tatoeba.org/exports/sentences.tar.bz2
  ```

#### 4. **æ—¥æœ¬èªæ–‡æ³•è¾å…¸ (è¯­æ³•æ•°æ®)**
- **ç½‘å€**: https://github.com/asdfjkl/jgram
- **å†…å®¹**: N5-N1è¯­æ³•ç‚¹,å¸¦ä¾‹å¥
- **æ ¼å¼**: JSON

---

## ğŸ•·ï¸ æ–¹æ³•äºŒ: çˆ¬å–åœ¨çº¿è¯å…¸ç½‘ç«™ (éœ€è¦ç¼–ç¨‹)

### âš ï¸ æ³¨æ„äº‹é¡¹:
- éµå®ˆç½‘ç«™çš„robots.txt
- æ§åˆ¶çˆ¬å–é€Ÿåº¦,é¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
- ä»…ç”¨äºä¸ªäººå­¦ä¹ ,ä¸ç”¨äºå•†ä¸šç”¨é€”

### æ¨èçˆ¬å–ç›®æ ‡:

#### 1. **Jisho.org (æ—¥è‹±è¯å…¸)**
- **ç½‘å€**: https://jisho.org/
- **ç‰¹ç‚¹**: æœ‰API,åˆæ³•è°ƒç”¨
- **APIæ–‡æ¡£**: https://jisho.org/forum/54fefc1f6e73340b1f160000-is-there-any-kind-of-search-api

**Pythonç¤ºä¾‹**:
```python
import requests
import json
import time

def scrape_jisho(word):
    url = f"https://jisho.org/api/v1/search/words?keyword={word}"
    response = requests.get(url)
    return response.json()

# çˆ¬å–JLPT N5è¯æ±‡
n5_words = ["ä¼šã†", "é’ã„", "èµ¤ã„", "ç§‹", "é–‹ã‘ã‚‹"]
results = []

for word in n5_words:
    print(f"æ­£åœ¨çˆ¬å–: {word}")
    data = scrape_jisho(word)
    
    if data['data']:
        entry = data['data'][0]
        results.append({
            'expression': entry['japanese'][0]['word'],
            'reading': entry['japanese'][0]['reading'],
            'meaning': '; '.join(entry['senses'][0]['english_definitions']),
            'level': 'N5'
        })
    
    time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«

# ä¿å­˜ä¸ºJSON
with open('vocabulary.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"çˆ¬å–å®Œæˆ!å…±{len(results)}ä¸ªè¯æ±‡")
```

#### 2. **JLPT Sensei (JLPTå­¦ä¹ ç½‘ç«™)**
- **ç½‘å€**: https://jlptsensei.com/
- **å†…å®¹**: æŒ‰ç­‰çº§åˆ†ç±»çš„è¯æ±‡å’Œè¯­æ³•

**Pythonçˆ¬è™«ç¤ºä¾‹**:
```python
import requests
from bs4 import BeautifulSoup
import json
import time

def scrape_jlpt_sensei_vocab(level='n5'):
    url = f"https://jlptsensei.com/jlpt-{level}-vocabulary-list/"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    vocab_list = []
    
    # æ‰¾åˆ°è¯æ±‡è¡¨æ ¼
    table = soup.find('table', class_='jl-table')
    if table:
        rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
        
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 3:
                vocab_list.append({
                    'expression': cols[0].text.strip(),
                    'reading': cols[1].text.strip(),
                    'meaning': cols[2].text.strip(),
                    'level': level.upper()
                })
    
    return vocab_list

# çˆ¬å–N5-N1è¯æ±‡
all_vocab = []
for level in ['n5', 'n4', 'n3', 'n2', 'n1']:
    print(f"æ­£åœ¨çˆ¬å– {level.upper()} è¯æ±‡...")
    vocab = scrape_jlpt_sensei_vocab(level)
    all_vocab.extend(vocab)
    print(f"  å®Œæˆ! è·å–{len(vocab)}ä¸ªè¯æ±‡")
    time.sleep(2)

# ä¿å­˜
with open('jlpt_vocabulary.json', 'w', encoding='utf-8') as f:
    json.dump(all_vocab, f, ensure_ascii=False, indent=2)

print(f"æ€»å…±çˆ¬å– {len(all_vocab)} ä¸ªè¯æ±‡")
```

#### 3. **æ—¥æœ¬èªã®ä¾‹æ–‡ (ä¾‹å¥ç½‘ç«™)**
- **ç½‘å€**: https://yourei.jp/
- **å†…å®¹**: å¤§é‡çœŸå®ä¾‹å¥

---

## ğŸ¤– æ–¹æ³•ä¸‰: ä½¿ç”¨AIç”Ÿæˆæ•°æ® (æœ€çµæ´»)

### ä½¿ç”¨åœºæ™¯:
- è¡¥å……ç¼ºå¤±çš„æ•°æ®
- ç”Ÿæˆä¾‹å¥
- æ‰©å±•é‡Šä¹‰

### ç¤ºä¾‹: ä½¿ç”¨OpenAI APIç”Ÿæˆè¯æ±‡æ•°æ®

**Pythonç¤ºä¾‹**:
```python
import openai
import json

openai.api_key = 'your-api-key'

def generate_vocabulary_data(word_list, level):
    prompt = f"""
    è¯·ä¸ºä»¥ä¸‹{level}çº§åˆ«çš„æ—¥è¯­è¯æ±‡ç”Ÿæˆè¯¦ç»†ä¿¡æ¯,è¿”å›JSONæ ¼å¼:
    
    è¯æ±‡åˆ—è¡¨: {', '.join(word_list)}
    
    æ¯ä¸ªè¯æ±‡éœ€è¦åŒ…å«:
    - expression: æ—¥æ–‡è¡¨è¾¾
    - reading: å‡åè¯»éŸ³
    - romaji: ç½—é©¬éŸ³
    - meaning: ä¸­æ–‡é‡Šä¹‰
    - partOfSpeech: è¯æ€§
    - examples: 2ä¸ªä¾‹å¥(æ—¥æ–‡ã€å‡åã€ä¸­æ–‡)
    
    è¿”å›JSONæ•°ç»„æ ¼å¼ã€‚
    """
    
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯æ—¥è¯­æ•™å­¦ä¸“å®¶"},
            {"role": "user", "content": prompt}
        ],
        response_format={"type": "json_object"}
    )
    
    return json.loads(response.choices[0].message.content)

# ä½¿ç”¨
words = ["ä¼šã†", "é’ã„", "èµ¤ã„"]
data = generate_vocabulary_data(words, "N5")

with open('ai_generated_vocab.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```

---

## ğŸ› ï¸ æ–¹æ³•å››: å®Œæ•´çˆ¬è™«é¡¹ç›® (ä¸“ä¸šçº§)

### ä½¿ç”¨Scrapyæ¡†æ¶

**å®‰è£…**:
```bash
pip install scrapy
```

**åˆ›å»ºé¡¹ç›®**:
```bash
scrapy startproject jlpt_scraper
cd jlpt_scraper
```

**åˆ›å»ºçˆ¬è™«** (`spiders/vocabulary_spider.py`):
```python
import scrapy
import json

class VocabularySpider(scrapy.Spider):
    name = 'vocabulary'
    start_urls = [
        'https://jlptsensei.com/jlpt-n5-vocabulary-list/',
        'https://jlptsensei.com/jlpt-n4-vocabulary-list/',
    ]
    
    def parse(self, response):
        # æå–JLPTç­‰çº§
        level = response.url.split('jlpt-')[1].split('-')[0].upper()
        
        # è§£æè¡¨æ ¼
        for row in response.css('table.jl-table tr')[1:]:
            cols = row.css('td::text').getall()
            if len(cols) >= 3:
                yield {
                    'expression': cols[0].strip(),
                    'reading': cols[1].strip(),
                    'meaning': cols[2].strip(),
                    'level': level
                }

# è¿è¡Œçˆ¬è™«
# scrapy crawl vocabulary -o vocabulary.json
```

---

## ğŸ“Š æ•°æ®å¤„ç†å’Œæ¸…æ´—

çˆ¬å–åˆ°æ•°æ®å,éœ€è¦æ¸…æ´—å’Œæ ‡å‡†åŒ–:

**Pythonæ¸…æ´—è„šæœ¬**:
```python
import json
import re

def clean_vocabulary_data(input_file, output_file):
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    cleaned_data = []
    seen = set()
    
    for item in data:
        # å»é‡
        if item['expression'] in seen:
            continue
        seen.add(item['expression'])
        
        # æ¸…æ´—æ•°æ®
        cleaned_item = {
            'expression': item['expression'].strip(),
            'reading': re.sub(r'[^\u3040-\u309F]', '', item['reading']),  # åªä¿ç•™å¹³å‡å
            'meaning': item['meaning'].strip(),
            'level': item.get('level', 'N5').upper()
        }
        
        # éªŒè¯å¿…å¡«å­—æ®µ
        if all([cleaned_item['expression'], 
                cleaned_item['reading'], 
                cleaned_item['meaning']]):
            cleaned_data.append(cleaned_item)
    
    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
    
    print(f"æ¸…æ´—å®Œæˆ! åŸå§‹: {len(data)}, æ¸…æ´—å: {len(cleaned_data)}")

# ä½¿ç”¨
clean_vocabulary_data('raw_vocabulary.json', 'cleaned_vocabulary.json')
```

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å¯¹äºéç¨‹åºå‘˜:
1. **ä¸‹è½½ç°æˆæ•°æ®é›†** (JMdict, GitHubå¼€æºé¡¹ç›®)
2. **ä½¿ç”¨åœ¨çº¿è½¬æ¢å·¥å…·** å°†XMLè½¬ä¸ºJSON
3. **ä¸Šä¼ åˆ°åº”ç”¨** é€šè¿‡ `/admin/import` å¯¼å…¥

### å¯¹äºåˆå­¦è€…ç¨‹åºå‘˜:
1. **ä½¿ç”¨Jisho.org API** (æœ‰å®˜æ–¹API,åˆæ³•)
2. **ç¼–å†™ç®€å•Pythonè„šæœ¬** è°ƒç”¨APIè·å–æ•°æ®
3. **ä¿å­˜ä¸ºJSON** ç„¶åå¯¼å…¥åº”ç”¨

### å¯¹äºæœ‰ç»éªŒçš„ç¨‹åºå‘˜:
1. **ä½¿ç”¨Scrapyæ¡†æ¶** çˆ¬å–å¤šä¸ªç½‘ç«™
2. **æ•°æ®æ¸…æ´—å’Œå»é‡**
3. **æ‰¹é‡å¯¼å…¥æ•°æ®åº“**

---

## ğŸ“ å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹

### åœºæ™¯: è·å–N5-N1å…¨éƒ¨è¯æ±‡

**ç¬¬1æ­¥: ä¸‹è½½JMdictæ•°æ®**
```bash
wget http://ftp.edrdg.org/pub/Nihongo/JMdict_e.gz
gunzip JMdict_e.gz
```

**ç¬¬2æ­¥: è§£æXMLå¹¶ç­›é€‰JLPTè¯æ±‡**
```python
import xml.etree.ElementTree as ET
import json

def parse_jmdict(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    
    vocab_list = []
    
    for entry in root.findall('entry'):
        # æå–æ—¥æ–‡
        kanji = entry.find('.//keb')
        reading = entry.find('.//reb')
        
        # æå–é‡Šä¹‰
        senses = entry.findall('.//sense')
        meanings = []
        for sense in senses:
            glosses = sense.findall('gloss')
            meanings.extend([g.text for g in glosses if g.text])
        
        # æå–JLPTç­‰çº§
        misc = entry.find('.//misc')
        level = None
        if misc is not None and 'jlpt' in misc.text.lower():
            level = misc.text
        
        if kanji is not None and reading is not None and meanings:
            vocab_list.append({
                'expression': kanji.text,
                'reading': reading.text,
                'meaning': '; '.join(meanings[:3]),
                'level': level or 'N5'
            })
    
    return vocab_list

# è§£æå¹¶ä¿å­˜
vocab = parse_jmdict('JMdict_e')
with open('jmdict_vocabulary.json', 'w', encoding='utf-8') as f:
    json.dump(vocab, f, ensure_ascii=False, indent=2)
```

**ç¬¬3æ­¥: å¯¼å…¥åˆ°åº”ç”¨**
- æ–¹å¼1: é€šè¿‡ `/admin/import` ä¸Šä¼ JSON
- æ–¹å¼2: è¿è¡Œå¯¼å…¥è„šæœ¬
- æ–¹å¼3: åœ¨Manuså¯¹è¯ä¸­è¯·æ±‚å¯¼å…¥

---

## âš–ï¸ æ³•å¾‹å’Œé“å¾·è€ƒè™‘

### âœ… åˆæ³•çš„æ•°æ®æ¥æº:
- å¼€æºæ•°æ®é›† (JMdict, Tatoebaç­‰)
- æœ‰å…¬å¼€APIçš„ç½‘ç«™ (Jisho.org)
- è‡ªå·±åˆ›å»ºçš„æ•°æ®
- AIç”Ÿæˆçš„æ•°æ®

### âš ï¸ éœ€è¦æ³¨æ„:
- éµå®ˆç½‘ç«™çš„robots.txt
- ä¸è¦è¿‡åº¦è¯·æ±‚,é¿å…DDoS
- æ ‡æ³¨æ•°æ®æ¥æº
- ä»…ç”¨äºä¸ªäººå­¦ä¹ ,ä¸å•†ç”¨

### âŒ ä¸æ¨è:
- çˆ¬å–æ˜ç¡®ç¦æ­¢çš„ç½‘ç«™
- å•†ä¸šåŒ–ä½¿ç”¨ä»–äººæ•°æ®
- ä¸æ ‡æ³¨æ¥æº

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

**å¦‚æœæ‚¨æƒ³ç«‹å³è·å–æ•°æ®**:

### æ–¹æ¡ˆA: æˆ‘å¸®æ‚¨çˆ¬å– (æœ€å¿«)
ç›´æ¥å‘Šè¯‰æˆ‘:
```
"å¸®æˆ‘çˆ¬å–N5-N1çš„è¯æ±‡æ•°æ®"
```
æˆ‘ä¼šç«‹å³æ‰§è¡Œçˆ¬è™«å¹¶å¯¼å…¥æ•°æ®åº“!

### æ–¹æ¡ˆB: ä½¿ç”¨ç°æˆæ•°æ® (æœ€ç®€å•)
1. è®¿é—® https://github.com/stephenmk/JMdictDB
2. ä¸‹è½½JSONæ–‡ä»¶
3. ä¸Šä¼ åˆ° `/admin/import`

### æ–¹æ¡ˆC: è‡ªå·±ç¼–å†™çˆ¬è™« (æœ€çµæ´»)
1. é€‰æ‹©ä¸Šé¢çš„Pythonç¤ºä¾‹
2. ä¿®æ”¹ç›®æ ‡ç½‘ç«™å’Œå­—æ®µ
3. è¿è¡Œè„šæœ¬è·å–æ•°æ®

---

## ğŸ’¡ æ¨èèµ„æº

### æ•°æ®æº:
- JMdict: http://www.edrdg.org/jmdict/j_jmdict.html
- Tatoeba: https://tatoeba.org/zh-hans/downloads
- JLPTè¯æ±‡: https://github.com/topics/jlpt-vocabulary

### å·¥å…·:
- Python + Requests + BeautifulSoup (ç®€å•çˆ¬è™«)
- Scrapy (ä¸“ä¸šçˆ¬è™«æ¡†æ¶)
- Jisho.org API (å®˜æ–¹æ¥å£)

### å­¦ä¹ èµ„æº:
- Pythonçˆ¬è™«æ•™ç¨‹: https://realpython.com/python-web-scraping-practical-introduction/
- Scrapyæ–‡æ¡£: https://docs.scrapy.org/

---

éœ€è¦æˆ‘ç°åœ¨å¸®æ‚¨çˆ¬å–æ•°æ®å—?æˆ–è€…æ‚¨æƒ³è‡ªå·±å°è¯•ç¼–å†™çˆ¬è™«?
